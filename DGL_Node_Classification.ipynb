{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hli8nova/DGL-KDD20-Hands-on-Tutorial/blob/master/DGL_Node_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1op-CbyLuN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1075d42-e2db-428d-c903-cb47c9a24585"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre dgl -f https://data.dgl.ai/wheels/cu116/repo.html\n",
        "!pip install --pre dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKYmWaq9beoj",
        "outputId": "8dcc0b83-f74c-483c-d11a-1e7e500ff920"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels/cu116/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.8/dist-packages (1.0.0+cu116)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.8/dist-packages (from dgl) (3.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from dgl) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (2.25.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Requirement already satisfied: dglgo in /usr/local/lib/python3.8/dist-packages (0.0.2)\n",
            "Requirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dglgo) (0.7.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.20 in /usr/local/lib/python3.8/dist-packages (from dglgo) (0.17.21)\n",
            "Requirement already satisfied: ogb>=1.3.3 in /usr/local/lib/python3.8/dist-packages (from dglgo) (1.3.5)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.8/dist-packages (from dglgo) (2022.9.4)\n",
            "Requirement already satisfied: isort>=5.10.1 in /usr/local/lib/python3.8/dist-packages (from dglgo) (6.0.0b2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.8/dist-packages (from dglgo) (6.0)\n",
            "Requirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from dglgo) (1.10.4)\n",
            "Requirement already satisfied: autopep8>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from dglgo) (2.0.1)\n",
            "Requirement already satisfied: numpydoc>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from dglgo) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from dglgo) (1.0.2)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.8/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: pycodestyle>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from autopep8>=1.6.0->dglgo) (2.10.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.8/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: sphinx>=4.2 in /usr/local/lib/python3.8/dist-packages (from numpydoc>=1.1.0->dglgo) (6.1.3)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (4.64.1)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (1.13.1+cu116)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (1.3.5)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ogb>=1.3.3->dglgo) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic>=1.9.0->dglgo) (4.5.0)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml>=0.17.20->dglgo) (0.2.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer>=0.4.0->dglgo) (7.1.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from rdkit-pypi->dglgo) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.25.1)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (57.4.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (0.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.11.0)\n",
            "Requirement already satisfied: docutils<0.20,>=0.18 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.19)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (6.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.13.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszt2RUHE7lW"
      },
      "source": [
        "# Node Classification with Graph Neural Networks\n",
        "\n",
        "[Previous: Introduction: Hands-on Graph Neural Networks](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8)\n",
        "\n",
        "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
        "\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total).\n",
        "\n",
        "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
        "We can make use [DGL] for an easy access to this dataset via [`dgl.data.CoraGraphDataset`](https://docs.dgl.ai/en/0.9.x/generated/dgl.data.CoraGraphDataset.html#dgl.data.CoraGraphDataset):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "rCOvXq0Ndj8o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl.data\n",
        "\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "print('Number of categories:', dataset.num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0C37cvgdsSc",
        "outputId": "6d82be17-5e75-4f39-ca95-88491c7d7a3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Number of categories: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGrKO5YH11-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faebacf8-f009-4f97-dff7-3e13a230f8be"
      },
      "source": [
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "features = data.ndata['feat']\n",
        "number_features = list(features.size())[1]\n",
        "print(f'Number of features: {number_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes()}')\n",
        "print(f'Number of edges: {data.num_edges()}')\n",
        "average_node_degree = data.num_edges() / data.num_nodes()\n",
        "print(f'Average node degree: {average_node_degree:.2f}')\n",
        "\n",
        "train_mask = data.ndata['train_mask']\n",
        "print(f'Number of training nodes: {train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(train_mask.sum()) / data.num_nodes():.2f}')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: Dataset(\"cora_v2\", num_graphs=1, save_path=/root/.dgl/cora_v2):\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={})\n",
            "===========================================================================================================\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Average node degree: 3.90\n",
            "Number of training nodes: 140\n",
            "Training node label rate: 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqWR0j_kIx67"
      },
      "source": [
        "We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n",
        "For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n",
        "This results in a training node label rate of only 5%.\n",
        "\n",
        "This graph holds the attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IRdAELVKOl6"
      },
      "source": [
        "## Training a Multi-layer Perception Network (MLP)\n",
        "\n",
        "In theory, we should be able to infer the category of a document solely based on its content, *i.e.* its bag-of-words feature representation, without taking any relational information into account.\n",
        "\n",
        "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afXwPCA3KNoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897fe490-c339-4806-b062-5e52f7545f25"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(number_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=1433, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_PO9EEHL7J6"
      },
      "source": [
        "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
        "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=16`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
        "\n",
        "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
        "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
        "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgHcLXMLk4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "72cb0faa-6b62-477c-ee7e-4f8b522aadb1"
      },
      "source": [
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MLP(hidden_channels=16)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
        "\n",
        "def train():\n",
        "      model.train()\n",
        "      optimizer.zero_grad()  # Clear gradients.\n",
        "      out = model(data.ndata['feat'])  # Perform a single forward pass.\n",
        "      loss = criterion(out[data.ndata['train_mask']], data.ndata['label'][data.ndata['train_mask']])  # Compute the loss solely based on the training nodes.\n",
        "      loss.backward()  # Derive gradients.\n",
        "      optimizer.step()  # Update parameters based on gradients.\n",
        "      return loss\n",
        "\n",
        "def test():\n",
        "      model.eval()\n",
        "      out = model(data.ndata['feat'])\n",
        "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "      test_correct = pred[data.ndata['test_mask']] == data.ndata['label'][data.ndata['test_mask']]  # Check against ground-truth labels.\n",
        "      test_acc = int(test_correct.sum()) / int(data.ndata['test_mask'].sum())  # Derive ratio of correct predictions.\n",
        "      return test_acc\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 1.9615\n",
            "Epoch: 002, Loss: 1.9557\n",
            "Epoch: 003, Loss: 1.9505\n",
            "Epoch: 004, Loss: 1.9423\n",
            "Epoch: 005, Loss: 1.9327\n",
            "Epoch: 006, Loss: 1.9279\n",
            "Epoch: 007, Loss: 1.9144\n",
            "Epoch: 008, Loss: 1.9087\n",
            "Epoch: 009, Loss: 1.9023\n",
            "Epoch: 010, Loss: 1.8893\n",
            "Epoch: 011, Loss: 1.8776\n",
            "Epoch: 012, Loss: 1.8594\n",
            "Epoch: 013, Loss: 1.8457\n",
            "Epoch: 014, Loss: 1.8365\n",
            "Epoch: 015, Loss: 1.8280\n",
            "Epoch: 016, Loss: 1.7965\n",
            "Epoch: 017, Loss: 1.7984\n",
            "Epoch: 018, Loss: 1.7832\n",
            "Epoch: 019, Loss: 1.7495\n",
            "Epoch: 020, Loss: 1.7441\n",
            "Epoch: 021, Loss: 1.7188\n",
            "Epoch: 022, Loss: 1.7124\n",
            "Epoch: 023, Loss: 1.6785\n",
            "Epoch: 024, Loss: 1.6660\n",
            "Epoch: 025, Loss: 1.6119\n",
            "Epoch: 026, Loss: 1.6236\n",
            "Epoch: 027, Loss: 1.5827\n",
            "Epoch: 028, Loss: 1.5784\n",
            "Epoch: 029, Loss: 1.5524\n",
            "Epoch: 030, Loss: 1.5020\n",
            "Epoch: 031, Loss: 1.5065\n",
            "Epoch: 032, Loss: 1.4742\n",
            "Epoch: 033, Loss: 1.4581\n",
            "Epoch: 034, Loss: 1.4246\n",
            "Epoch: 035, Loss: 1.4131\n",
            "Epoch: 036, Loss: 1.4112\n",
            "Epoch: 037, Loss: 1.3923\n",
            "Epoch: 038, Loss: 1.3055\n",
            "Epoch: 039, Loss: 1.2982\n",
            "Epoch: 040, Loss: 1.2543\n",
            "Epoch: 041, Loss: 1.2244\n",
            "Epoch: 042, Loss: 1.2331\n",
            "Epoch: 043, Loss: 1.1984\n",
            "Epoch: 044, Loss: 1.1796\n",
            "Epoch: 045, Loss: 1.1093\n",
            "Epoch: 046, Loss: 1.1284\n",
            "Epoch: 047, Loss: 1.1229\n",
            "Epoch: 048, Loss: 1.0383\n",
            "Epoch: 049, Loss: 1.0439\n",
            "Epoch: 050, Loss: 1.0563\n",
            "Epoch: 051, Loss: 0.9893\n",
            "Epoch: 052, Loss: 1.0508\n",
            "Epoch: 053, Loss: 0.9343\n",
            "Epoch: 054, Loss: 0.9639\n",
            "Epoch: 055, Loss: 0.8929\n",
            "Epoch: 056, Loss: 0.8705\n",
            "Epoch: 057, Loss: 0.9176\n",
            "Epoch: 058, Loss: 0.9239\n",
            "Epoch: 059, Loss: 0.8641\n",
            "Epoch: 060, Loss: 0.8578\n",
            "Epoch: 061, Loss: 0.7908\n",
            "Epoch: 062, Loss: 0.7856\n",
            "Epoch: 063, Loss: 0.7683\n",
            "Epoch: 064, Loss: 0.7816\n",
            "Epoch: 065, Loss: 0.7356\n",
            "Epoch: 066, Loss: 0.6951\n",
            "Epoch: 067, Loss: 0.7300\n",
            "Epoch: 068, Loss: 0.6939\n",
            "Epoch: 069, Loss: 0.7550\n",
            "Epoch: 070, Loss: 0.6864\n",
            "Epoch: 071, Loss: 0.7094\n",
            "Epoch: 072, Loss: 0.7238\n",
            "Epoch: 073, Loss: 0.7150\n",
            "Epoch: 074, Loss: 0.6191\n",
            "Epoch: 075, Loss: 0.6770\n",
            "Epoch: 076, Loss: 0.6487\n",
            "Epoch: 077, Loss: 0.6258\n",
            "Epoch: 078, Loss: 0.5821\n",
            "Epoch: 079, Loss: 0.5637\n",
            "Epoch: 080, Loss: 0.6368\n",
            "Epoch: 081, Loss: 0.6333\n",
            "Epoch: 082, Loss: 0.6434\n",
            "Epoch: 083, Loss: 0.5974\n",
            "Epoch: 084, Loss: 0.6176\n",
            "Epoch: 085, Loss: 0.5972\n",
            "Epoch: 086, Loss: 0.4690\n",
            "Epoch: 087, Loss: 0.6362\n",
            "Epoch: 088, Loss: 0.6118\n",
            "Epoch: 089, Loss: 0.5248\n",
            "Epoch: 090, Loss: 0.5520\n",
            "Epoch: 091, Loss: 0.6130\n",
            "Epoch: 092, Loss: 0.5361\n",
            "Epoch: 093, Loss: 0.5594\n",
            "Epoch: 094, Loss: 0.5049\n",
            "Epoch: 095, Loss: 0.5043\n",
            "Epoch: 096, Loss: 0.5235\n",
            "Epoch: 097, Loss: 0.5451\n",
            "Epoch: 098, Loss: 0.5329\n",
            "Epoch: 099, Loss: 0.5008\n",
            "Epoch: 100, Loss: 0.5350\n",
            "Epoch: 101, Loss: 0.5343\n",
            "Epoch: 102, Loss: 0.5138\n",
            "Epoch: 103, Loss: 0.5377\n",
            "Epoch: 104, Loss: 0.5353\n",
            "Epoch: 105, Loss: 0.5176\n",
            "Epoch: 106, Loss: 0.5229\n",
            "Epoch: 107, Loss: 0.4558\n",
            "Epoch: 108, Loss: 0.4883\n",
            "Epoch: 109, Loss: 0.4659\n",
            "Epoch: 110, Loss: 0.4908\n",
            "Epoch: 111, Loss: 0.4966\n",
            "Epoch: 112, Loss: 0.4725\n",
            "Epoch: 113, Loss: 0.4787\n",
            "Epoch: 114, Loss: 0.4390\n",
            "Epoch: 115, Loss: 0.4199\n",
            "Epoch: 116, Loss: 0.4810\n",
            "Epoch: 117, Loss: 0.4484\n",
            "Epoch: 118, Loss: 0.5080\n",
            "Epoch: 119, Loss: 0.4241\n",
            "Epoch: 120, Loss: 0.4745\n",
            "Epoch: 121, Loss: 0.4651\n",
            "Epoch: 122, Loss: 0.4652\n",
            "Epoch: 123, Loss: 0.5580\n",
            "Epoch: 124, Loss: 0.4861\n",
            "Epoch: 125, Loss: 0.4405\n",
            "Epoch: 126, Loss: 0.4292\n",
            "Epoch: 127, Loss: 0.4409\n",
            "Epoch: 128, Loss: 0.3575\n",
            "Epoch: 129, Loss: 0.4468\n",
            "Epoch: 130, Loss: 0.4603\n",
            "Epoch: 131, Loss: 0.4108\n",
            "Epoch: 132, Loss: 0.4601\n",
            "Epoch: 133, Loss: 0.4258\n",
            "Epoch: 134, Loss: 0.3852\n",
            "Epoch: 135, Loss: 0.4028\n",
            "Epoch: 136, Loss: 0.4245\n",
            "Epoch: 137, Loss: 0.4300\n",
            "Epoch: 138, Loss: 0.4693\n",
            "Epoch: 139, Loss: 0.4314\n",
            "Epoch: 140, Loss: 0.4031\n",
            "Epoch: 141, Loss: 0.4290\n",
            "Epoch: 142, Loss: 0.4110\n",
            "Epoch: 143, Loss: 0.3863\n",
            "Epoch: 144, Loss: 0.4215\n",
            "Epoch: 145, Loss: 0.4519\n",
            "Epoch: 146, Loss: 0.3940\n",
            "Epoch: 147, Loss: 0.4429\n",
            "Epoch: 148, Loss: 0.3527\n",
            "Epoch: 149, Loss: 0.4390\n",
            "Epoch: 150, Loss: 0.4212\n",
            "Epoch: 151, Loss: 0.4128\n",
            "Epoch: 152, Loss: 0.3779\n",
            "Epoch: 153, Loss: 0.4801\n",
            "Epoch: 154, Loss: 0.4130\n",
            "Epoch: 155, Loss: 0.3962\n",
            "Epoch: 156, Loss: 0.4262\n",
            "Epoch: 157, Loss: 0.4210\n",
            "Epoch: 158, Loss: 0.4081\n",
            "Epoch: 159, Loss: 0.4066\n",
            "Epoch: 160, Loss: 0.3782\n",
            "Epoch: 161, Loss: 0.3836\n",
            "Epoch: 162, Loss: 0.4172\n",
            "Epoch: 163, Loss: 0.3993\n",
            "Epoch: 164, Loss: 0.4477\n",
            "Epoch: 165, Loss: 0.3714\n",
            "Epoch: 166, Loss: 0.3610\n",
            "Epoch: 167, Loss: 0.4546\n",
            "Epoch: 168, Loss: 0.4387\n",
            "Epoch: 169, Loss: 0.3793\n",
            "Epoch: 170, Loss: 0.3704\n",
            "Epoch: 171, Loss: 0.4286\n",
            "Epoch: 172, Loss: 0.4131\n",
            "Epoch: 173, Loss: 0.3795\n",
            "Epoch: 174, Loss: 0.4230\n",
            "Epoch: 175, Loss: 0.4139\n",
            "Epoch: 176, Loss: 0.3586\n",
            "Epoch: 177, Loss: 0.3588\n",
            "Epoch: 178, Loss: 0.3911\n",
            "Epoch: 179, Loss: 0.3810\n",
            "Epoch: 180, Loss: 0.4203\n",
            "Epoch: 181, Loss: 0.3583\n",
            "Epoch: 182, Loss: 0.3690\n",
            "Epoch: 183, Loss: 0.4025\n",
            "Epoch: 184, Loss: 0.3920\n",
            "Epoch: 185, Loss: 0.4369\n",
            "Epoch: 186, Loss: 0.4317\n",
            "Epoch: 187, Loss: 0.4911\n",
            "Epoch: 188, Loss: 0.3369\n",
            "Epoch: 189, Loss: 0.4945\n",
            "Epoch: 190, Loss: 0.3912\n",
            "Epoch: 191, Loss: 0.3824\n",
            "Epoch: 192, Loss: 0.3479\n",
            "Epoch: 193, Loss: 0.3798\n",
            "Epoch: 194, Loss: 0.3799\n",
            "Epoch: 195, Loss: 0.4015\n",
            "Epoch: 196, Loss: 0.3615\n",
            "Epoch: 197, Loss: 0.3985\n",
            "Epoch: 198, Loss: 0.4664\n",
            "Epoch: 199, Loss: 0.3714\n",
            "Epoch: 200, Loss: 0.3810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG4IKy9YOLGF"
      },
      "source": [
        "After training the model, we can call the `test` function to see how well our model performs on unseen labels.\n",
        "Here, we are interested in the accuracy of the model, *i.e.*, the ratio of correctly classified nodes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBBCeLlAL0oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b6a0b-c280-4afe-fa43-556f97f36cfd"
      },
      "source": [
        "test_acc = test()\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jjJOB-VO-cw"
      },
      "source": [
        "As one can see, our MLP performs rather bad with only about 59% test accuracy.\n",
        "But why does the MLP do not perform better?\n",
        "The main reason for that is that this model suffers from heavy overfitting due to only having access to a **small amount of training nodes**, and therefore generalizes poorly to unseen node representations.\n",
        "\n",
        "It also fails to incorporate an important bias into the model: **Cited papers are very likely related to the category of a document**.\n",
        "That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWGw54wRd98"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with DGL's GNN operators.\n",
        "\n",
        "Following-up on [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the GCN module.\n",
        "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
        "In contrast, a single `Linear` layer is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
        "$$\n",
        "\n",
        "which does not make use of neighboring node information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = dataset[0]\n",
        "print('Node features')\n",
        "print(g.ndata)\n",
        "print('Edge features')\n",
        "print(g.edata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txT3dIFxnVfK",
        "outputId": "0a9aa614-4df0-43cd-8480-d29e2f00f90f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features\n",
            "{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n",
            "Edge features\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "# Create the model with given dimensions\n",
        "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
      ],
      "metadata": {
        "id": "Z0JBjBpwnfzA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the GNN"
      ],
      "metadata": {
        "id": "aD59uBP4nvHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g, model):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "\n",
        "    features = g.ndata['feat']\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    for e in range(100):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % 5 == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
        "\n",
        "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
        "train(g, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNcwF2f6nme8",
        "outputId": "a0b82a1f-0ee5-43d1-b9a7-8ffd62de523c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.947, val acc: 0.206 (best 0.206), test acc: 0.198 (best 0.198)\n",
            "In epoch 5, loss: 1.904, val acc: 0.628 (best 0.628), test acc: 0.614 (best 0.614)\n",
            "In epoch 10, loss: 1.837, val acc: 0.674 (best 0.686), test acc: 0.716 (best 0.715)\n",
            "In epoch 15, loss: 1.746, val acc: 0.674 (best 0.686), test acc: 0.685 (best 0.715)\n",
            "In epoch 20, loss: 1.625, val acc: 0.686 (best 0.690), test acc: 0.714 (best 0.704)\n",
            "In epoch 25, loss: 1.484, val acc: 0.712 (best 0.712), test acc: 0.727 (best 0.727)\n",
            "In epoch 30, loss: 1.322, val acc: 0.756 (best 0.756), test acc: 0.757 (best 0.755)\n",
            "In epoch 35, loss: 1.148, val acc: 0.752 (best 0.760), test acc: 0.755 (best 0.761)\n",
            "In epoch 40, loss: 0.972, val acc: 0.748 (best 0.760), test acc: 0.746 (best 0.761)\n",
            "In epoch 45, loss: 0.804, val acc: 0.760 (best 0.760), test acc: 0.759 (best 0.761)\n",
            "In epoch 50, loss: 0.653, val acc: 0.758 (best 0.760), test acc: 0.762 (best 0.761)\n",
            "In epoch 55, loss: 0.524, val acc: 0.764 (best 0.764), test acc: 0.762 (best 0.762)\n",
            "In epoch 60, loss: 0.419, val acc: 0.770 (best 0.770), test acc: 0.765 (best 0.765)\n",
            "In epoch 65, loss: 0.336, val acc: 0.768 (best 0.770), test acc: 0.772 (best 0.765)\n",
            "In epoch 70, loss: 0.270, val acc: 0.770 (best 0.770), test acc: 0.773 (best 0.765)\n",
            "In epoch 75, loss: 0.220, val acc: 0.772 (best 0.774), test acc: 0.770 (best 0.772)\n",
            "In epoch 80, loss: 0.181, val acc: 0.774 (best 0.776), test acc: 0.770 (best 0.771)\n",
            "In epoch 85, loss: 0.150, val acc: 0.776 (best 0.776), test acc: 0.771 (best 0.771)\n",
            "In epoch 90, loss: 0.126, val acc: 0.780 (best 0.780), test acc: 0.770 (best 0.770)\n",
            "In epoch 95, loss: 0.107, val acc: 0.780 (best 0.780), test acc: 0.769 (best 0.770)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpdscco5g6kG"
      },
      "source": [
        "We certainly can do better by training our model.\n",
        "The training and testing procedure is once again the same, but this time we make use of the node features `x` **and** the graph connectivity `edge_index` as input to our GCN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opBBGQHqg5ZO"
      },
      "source": [
        "After training the model, we can check its test accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhofzjaqhfY2"
      },
      "source": [
        "**There it is!**\n",
        "By simply swapping the linear layers with GNN layers, we can reach **77% of test accuracy**!\n",
        "This is in stark contrast to the 59% of test accuracy obtained by our MLP, indicating that relational information plays a crucial role in obtaining better performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specific GPU for training to get much better speed for large dataset.\n",
        "- Note, the total clock time may not improve much for smaller datasets due to overheads of sending to and receiving data from GPU. "
      ],
      "metadata": {
        "id": "_Pe--xYXwcpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = g.to('cuda')\n",
        "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
        "train(g, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6EY80zdwXmi",
        "outputId": "cb851e4b-c3b1-4b4f-8781-165407f7cee0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.946, val acc: 0.092 (best 0.092), test acc: 0.085 (best 0.085)\n",
            "In epoch 5, loss: 1.884, val acc: 0.400 (best 0.400), test acc: 0.384 (best 0.384)\n",
            "In epoch 10, loss: 1.795, val acc: 0.534 (best 0.534), test acc: 0.565 (best 0.565)\n",
            "In epoch 15, loss: 1.683, val acc: 0.586 (best 0.586), test acc: 0.637 (best 0.637)\n",
            "In epoch 20, loss: 1.549, val acc: 0.630 (best 0.630), test acc: 0.670 (best 0.670)\n",
            "In epoch 25, loss: 1.399, val acc: 0.680 (best 0.680), test acc: 0.701 (best 0.701)\n",
            "In epoch 30, loss: 1.237, val acc: 0.698 (best 0.698), test acc: 0.717 (best 0.717)\n",
            "In epoch 35, loss: 1.071, val acc: 0.700 (best 0.700), test acc: 0.739 (best 0.739)\n",
            "In epoch 40, loss: 0.909, val acc: 0.714 (best 0.714), test acc: 0.746 (best 0.746)\n",
            "In epoch 45, loss: 0.756, val acc: 0.738 (best 0.738), test acc: 0.758 (best 0.758)\n",
            "In epoch 50, loss: 0.620, val acc: 0.740 (best 0.740), test acc: 0.759 (best 0.759)\n",
            "In epoch 55, loss: 0.503, val acc: 0.746 (best 0.746), test acc: 0.766 (best 0.766)\n",
            "In epoch 60, loss: 0.406, val acc: 0.754 (best 0.754), test acc: 0.770 (best 0.770)\n",
            "In epoch 65, loss: 0.328, val acc: 0.766 (best 0.766), test acc: 0.778 (best 0.778)\n",
            "In epoch 70, loss: 0.266, val acc: 0.766 (best 0.768), test acc: 0.780 (best 0.783)\n",
            "In epoch 75, loss: 0.217, val acc: 0.766 (best 0.768), test acc: 0.779 (best 0.783)\n",
            "In epoch 80, loss: 0.179, val acc: 0.764 (best 0.768), test acc: 0.779 (best 0.783)\n",
            "In epoch 85, loss: 0.149, val acc: 0.770 (best 0.770), test acc: 0.781 (best 0.781)\n",
            "In epoch 90, loss: 0.126, val acc: 0.768 (best 0.770), test acc: 0.782 (best 0.781)\n",
            "In epoch 95, loss: 0.107, val acc: 0.772 (best 0.772), test acc: 0.782 (best 0.781)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paMH3_7ejSg4"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this chapter, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model's performance.\n",
        "In the next section, we will look into how GNNs can be used for the task of graph classification.\n",
        "\n",
        "[Next: Graph Classification with Graph Neural Networks](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb)"
      ]
    }
  ]
}