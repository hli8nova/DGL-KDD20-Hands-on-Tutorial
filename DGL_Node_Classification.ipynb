{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hli8nova/DGL-KDD20-Hands-on-Tutorial/blob/master/DGL_Node_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1op-CbyLuN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9908349-d69b-430d-84be-98efd16acbfb"
      },
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dgl -f https://data.dgl.ai/wheels/cu116/repo.html\n",
        "!pip install -q dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ],
      "metadata": {
        "id": "tKYmWaq9beoj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszt2RUHE7lW"
      },
      "source": [
        "# Node Classification with Graph Neural Networks\n",
        "\n",
        "[Previous: Introduction: Hands-on Graph Neural Networks](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8)\n",
        "\n",
        "This tutorial will teach you how to apply **Graph Neural Networks (GNNs) to the task of node classification**.\n",
        "Here, we are given the ground-truth labels of only a small subset of nodes, and want to infer the labels for all the remaining nodes (*transductive learning*).\n",
        "\n",
        "To demonstrate, we make use of the `Cora` dataset, which is a **citation network** where nodes represent documents.\n",
        "Each node is described by a 1433-dimensional bag-of-words feature vector.\n",
        "Two documents are connected if there exists a citation link between them.\n",
        "The task is to infer the category of each document (7 in total).\n",
        "\n",
        "This dataset was first introduced by [Yang et al. (2016)](https://arxiv.org/abs/1603.08861) as one of the datasets of the `Planetoid` benchmark suite.\n",
        "We can make use [DGL] for an easy access to this dataset via [`dgl.data.CoraGraphDataset`](https://docs.dgl.ai/en/0.9.x/generated/dgl.data.CoraGraphDataset.html#dgl.data.CoraGraphDataset):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "rCOvXq0Ndj8o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl.data\n",
        "\n",
        "dataset = dgl.data.CoraGraphDataset()\n",
        "print('Number of categories:', dataset.num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0C37cvgdsSc",
        "outputId": "abae4639-4e5e-4ac8-abf3-8e3e19112396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n",
            "Number of categories: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imGrKO5YH11-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b325507-5a56-4ff0-fd71-90d37c366051"
      },
      "source": [
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "g = dataset[0]  # Get the first graph object.\n",
        "print(g)\n",
        "\n",
        "print('\\nNode features:')\n",
        "print(g.ndata)\n",
        "print('\\nEdge features:')\n",
        "print(g.edata)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: Dataset(\"cora_v2\", num_graphs=1, save_path=/root/.dgl/cora_v2):\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Graph(num_nodes=2708, num_edges=10556,\n",
            "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
            "      edata_schemes={})\n",
            "\n",
            "Node features:\n",
            "{'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n",
            "\n",
            "Edge features:\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = g.ndata['feat']\n",
        "number_features = list(g.ndata['feat'].size())[1]\n",
        "\n",
        "print(f'Number of features: {number_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "print()\n",
        "print('===========================================================================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {g.num_nodes()}')\n",
        "print(f'Number of edges: {g.num_edges()}')\n",
        "average_node_degree = g.num_edges() / g.num_nodes()\n",
        "print(f'Average node degree: {average_node_degree:.2f}')\n",
        "\n",
        "train_mask = g.ndata['train_mask']\n",
        "print(f'Number of training nodes: {train_mask.sum()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91BD3VamOc4N",
        "outputId": "d8e0427c-6951-4241-cd9f-2f0db318bbcf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "\n",
            "===========================================================================================================\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Average node degree: 3.90\n",
            "Number of training nodes: 140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqWR0j_kIx67"
      },
      "source": [
        "We can see that the `Cora` network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9.\n",
        "For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class).\n",
        "\n",
        "This graph holds the attributes `val_mask` and `test_mask`, which denotes which nodes should be used for validation and testing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IRdAELVKOl6"
      },
      "source": [
        "## Training a Multi-layer Perception Network (MLP)\n",
        "\n",
        "In theory, we should be able to infer the category of a document solely based on its content, *i.e.* its bag-of-words feature representation, without taking any relational information into account.\n",
        "\n",
        "Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afXwPCA3KNoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a02aece-98fc-4c8d-810f-0d3f60d4d220"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.lin1 = Linear(number_features, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.lin1(in_feat)\n",
        "        h = h.relu()\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.lin2(h)\n",
        "        return h\n",
        "\n",
        "model = MLP(hidden_channels=256)\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (lin1): Linear(in_features=1433, out_features=256, bias=True)\n",
            "  (lin2): Linear(in_features=256, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_PO9EEHL7J6"
      },
      "source": [
        "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity and [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout).\n",
        "Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding (`hidden_channels=128`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes.\n",
        "\n",
        "Let's train our simple MLP by following a similar procedure as described in [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8).\n",
        "We again make use of the **cross entropy loss** and **Adam optimizer**.\n",
        "This time, we also define a **`test` function** to evaluate how well our final model performs on the test node set (which labels have not been observed during training)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def train(g, model, epoch = 200):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    best_val_acc = 0\n",
        "    best_test_acc = 0\n",
        "    print_step = math.floor(epoch/10)\n",
        "\n",
        "    features = g.ndata['feat']\n",
        "    labels = g.ndata['label']\n",
        "    train_mask = g.ndata['train_mask']\n",
        "    val_mask = g.ndata['val_mask']\n",
        "    test_mask = g.ndata['test_mask']\n",
        "    for e in range(epoch+1):\n",
        "        # Forward\n",
        "        logits = model(g, features)\n",
        "\n",
        "        # Compute prediction\n",
        "        pred = logits.argmax(1)\n",
        "\n",
        "        # Compute loss\n",
        "        # Note that you should only compute the losses of the nodes in the training set.\n",
        "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
        "\n",
        "        # Compute accuracy on training/validation/test\n",
        "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
        "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
        "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
        "\n",
        "        # Save the best validation accuracy and the corresponding test accuracy.\n",
        "        if best_val_acc < val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if e % print_step == 0:\n",
        "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
        "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n"
      ],
      "metadata": {
        "id": "wUSczkP5LjWX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the module\n",
        "import time\n",
        "# records start time\n",
        "start = time.perf_counter()\n",
        "\n",
        "g = g.to('cuda')\n",
        "mlp_model = MLP(hidden_channels=256).to('cuda')\n",
        "train(g, mlp_model, 100)\n",
        "\n",
        "# record end time\n",
        "end = time.perf_counter()\n",
        " \n",
        "# find elapsed time in seconds\n",
        "ms = (end-start)\n",
        "print(f\"\\nElapsed {ms:.03f} secs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxNS0XzONQoO",
        "outputId": "56085606-7b65-4ae5-ca7d-91893da6e2f6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.946, val acc: 0.060 (best 0.060), test acc: 0.064 (best 0.064)\n",
            "In epoch 10, loss: 1.460, val acc: 0.446 (best 0.476), test acc: 0.449 (best 0.424)\n",
            "In epoch 20, loss: 0.465, val acc: 0.486 (best 0.512), test acc: 0.507 (best 0.482)\n",
            "In epoch 30, loss: 0.060, val acc: 0.508 (best 0.522), test acc: 0.510 (best 0.516)\n",
            "In epoch 40, loss: 0.012, val acc: 0.530 (best 0.530), test acc: 0.488 (best 0.488)\n",
            "In epoch 50, loss: 0.005, val acc: 0.522 (best 0.532), test acc: 0.500 (best 0.507)\n",
            "In epoch 60, loss: 0.004, val acc: 0.496 (best 0.532), test acc: 0.500 (best 0.507)\n",
            "In epoch 70, loss: 0.002, val acc: 0.510 (best 0.544), test acc: 0.516 (best 0.530)\n",
            "In epoch 80, loss: 0.002, val acc: 0.520 (best 0.544), test acc: 0.506 (best 0.530)\n",
            "In epoch 90, loss: 0.003, val acc: 0.512 (best 0.544), test acc: 0.513 (best 0.530)\n",
            "In epoch 100, loss: 0.002, val acc: 0.534 (best 0.544), test acc: 0.505 (best 0.530)\n",
            "\n",
            "Elapsed 0.414 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jjJOB-VO-cw"
      },
      "source": [
        "As one can see, our MLP performs rather bad with only about 53% test accuracy.\n",
        "But why does the MLP do not perform better?\n",
        "The main reason for that is that this model suffers from heavy overfitting due to only having access to a **small amount of training nodes**, and therefore generalizes poorly to unseen node representations.\n",
        "\n",
        "It also fails to incorporate an important bias into the model: **Cited papers are very likely related to the category of a document**.\n",
        "That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWGw54wRd98"
      },
      "source": [
        "## Training a Graph Neural Network (GNN)\n",
        "\n",
        "We can easily convert our MLP to a GNN by swapping the `torch.nn.Linear` layers with DGL's GNN operators.\n",
        "\n",
        "Following-up on [the first part of this tutorial](https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8), we replace the linear layers by the GCN module.\n",
        "To recap, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)) is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
        "In contrast, a single `Linear` layer is defined as\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}\n",
        "$$\n",
        "\n",
        "which does not make use of neighboring node information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn import GraphConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, num_classes)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "# Create the model with given dimensions\n",
        "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
      ],
      "metadata": {
        "id": "Z0JBjBpwnfzA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the GNN"
      ],
      "metadata": {
        "id": "aD59uBP4nvHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the module\n",
        "import time\n",
        "# records start time\n",
        "start = time.perf_counter()\n",
        "\n",
        "g = g.to('cpu')\n",
        "model = GCN(g.ndata['feat'].shape[1], 256, dataset.num_classes).to('cpu')\n",
        "train(g, model, 50)\n",
        "\n",
        "# record end time\n",
        "end = time.perf_counter()\n",
        " \n",
        "# find elapsed time in seconds\n",
        "ms = (end-start)\n",
        "print(f\"\\nElapsed {ms:.03f} secs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNcwF2f6nme8",
        "outputId": "b11b988a-e256-4f7e-9aad-40e634b6a95c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.947, val acc: 0.136 (best 0.136), test acc: 0.114 (best 0.114)\n",
            "In epoch 5, loss: 1.704, val acc: 0.618 (best 0.618), test acc: 0.633 (best 0.633)\n",
            "In epoch 10, loss: 1.273, val acc: 0.730 (best 0.730), test acc: 0.751 (best 0.751)\n",
            "In epoch 15, loss: 0.760, val acc: 0.760 (best 0.768), test acc: 0.774 (best 0.768)\n",
            "In epoch 20, loss: 0.392, val acc: 0.774 (best 0.778), test acc: 0.785 (best 0.783)\n",
            "In epoch 25, loss: 0.199, val acc: 0.784 (best 0.784), test acc: 0.783 (best 0.782)\n",
            "In epoch 30, loss: 0.096, val acc: 0.768 (best 0.784), test acc: 0.784 (best 0.782)\n",
            "In epoch 35, loss: 0.055, val acc: 0.788 (best 0.788), test acc: 0.782 (best 0.782)\n",
            "In epoch 40, loss: 0.029, val acc: 0.766 (best 0.788), test acc: 0.781 (best 0.782)\n",
            "In epoch 45, loss: 0.018, val acc: 0.762 (best 0.788), test acc: 0.776 (best 0.782)\n",
            "In epoch 50, loss: 0.014, val acc: 0.768 (best 0.790), test acc: 0.769 (best 0.774)\n",
            "\n",
            "Elapsed 4.174 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpdscco5g6kG"
      },
      "source": [
        "We certainly can do better by training our model.\n",
        "The training and testing procedure is once again the same, but this time we make use of the node features `x` **and** the graph connectivity as input to our GCN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhofzjaqhfY2"
      },
      "source": [
        "**There it is!**\n",
        "By simply swapping the linear layers with GNN layers, we can reach **78% of test accuracy**!\n",
        "This is in stark contrast to the 53% of test accuracy obtained by our MLP, indicating that relational information plays a crucial role in obtaining better performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also specific GPU for training to get much better speed for large dataset.\n",
        "- Note, the total clock time may not improve much for smaller datasets due to overheads of sending to and receiving data from GPU. "
      ],
      "metadata": {
        "id": "_Pe--xYXwcpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the module\n",
        "import time\n",
        "# records start time\n",
        "start = time.perf_counter()\n",
        "\n",
        "g = g.to('cuda')\n",
        "model = GCN(g.ndata['feat'].shape[1], 256, dataset.num_classes).to('cuda')\n",
        "train(g, model, 50)\n",
        "\n",
        "# record end time\n",
        "end = time.perf_counter()\n",
        " \n",
        "# find elapsed time in seconds\n",
        "ms = (end-start)\n",
        "print(f\"\\nElapsed {ms:.03f} secs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwvuwgWGRJr4",
        "outputId": "cf861d67-42f2-4bcf-d7a0-6539038143ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 1.946, val acc: 0.074 (best 0.074), test acc: 0.131 (best 0.131)\n",
            "In epoch 5, loss: 1.700, val acc: 0.604 (best 0.604), test acc: 0.625 (best 0.625)\n",
            "In epoch 10, loss: 1.268, val acc: 0.722 (best 0.722), test acc: 0.748 (best 0.748)\n",
            "In epoch 15, loss: 0.773, val acc: 0.754 (best 0.766), test acc: 0.770 (best 0.759)\n",
            "In epoch 20, loss: 0.389, val acc: 0.758 (best 0.776), test acc: 0.773 (best 0.784)\n",
            "In epoch 25, loss: 0.189, val acc: 0.778 (best 0.778), test acc: 0.779 (best 0.779)\n",
            "In epoch 30, loss: 0.096, val acc: 0.764 (best 0.786), test acc: 0.794 (best 0.787)\n",
            "In epoch 35, loss: 0.054, val acc: 0.786 (best 0.786), test acc: 0.786 (best 0.787)\n",
            "In epoch 40, loss: 0.027, val acc: 0.778 (best 0.786), test acc: 0.777 (best 0.787)\n",
            "In epoch 45, loss: 0.021, val acc: 0.774 (best 0.786), test acc: 0.778 (best 0.787)\n",
            "In epoch 50, loss: 0.015, val acc: 0.772 (best 0.786), test acc: 0.756 (best 0.787)\n",
            "\n",
            "Elapsed 0.339 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paMH3_7ejSg4"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this chapter, you have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model's performance.\n",
        "In the next section, we will look into how GNNs can be used for the task of graph classification.\n",
        "\n",
        "[Next: Graph Classification with Graph Neural Networks](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb)"
      ]
    }
  ]
}